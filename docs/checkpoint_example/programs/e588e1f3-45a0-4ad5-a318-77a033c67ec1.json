{"id": "e588e1f3-45a0-4ad5-a318-77a033c67ec1", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid global optimization using multistart simulated annealing, local search, and evolutionary jumps.\n    Further improves exploration and robustness in escaping local minima.\n    \"\"\"\n    n_starts = 30  # Increase number of random restarts for broader exploration\n    best_x, best_y, best_value = None, None, np.inf\n\n    for start in range(n_starts):\n        # Initialize with a random point for each restart\n        current_x = np.random.uniform(bounds[0], bounds[1])\n        current_y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(current_x, current_y)\n        \n        temperature = 2.0  # Slightly higher initial temperature for easier escape\n        cooling_rate = 0.993  # Slower cooling for more thorough exploration\n        min_temperature = 1e-10  # Lower minimum temperature for finer convergence\n\n        patience = 60  # Less patience before making a jump, encourages more exploration\n        no_improve_count = 0\n\n        for it in range(iterations // n_starts):\n            # Generate a new candidate point by adding a perturbation\n            perturbation_scale = max(0.8 * temperature, 0.02)  # Larger dynamic scale, min bound\n            new_x = current_x + np.random.normal(0, perturbation_scale)\n            new_y = current_y + np.random.normal(0, perturbation_scale)\n\n            # Enforce bounds\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n\n            new_value = evaluate_function(new_x, new_y)\n\n            # Acceptance criteria (Simulated Annealing)\n            if new_value < current_value:\n                accept = True\n            else:\n                accept = np.exp((current_value - new_value) / temperature) > np.random.rand()\n            \n            if accept:\n                current_x, current_y, current_value = new_x, new_y, new_value\n                no_improve_count = 0\n            else:\n                no_improve_count += 1\n\n            # Update best found so far\n            if current_value < best_value:\n                best_x, best_y, best_value = current_x, current_y, current_value\n                no_improve_count = 0\n\n            # Local search: more frequent and with momentum\n            if it % 7 == 0:\n                grad_x = (evaluate_function(current_x + 1e-4, current_y) - current_value) / 1e-4\n                grad_y = (evaluate_function(current_x, current_y + 1e-4) - current_value) / 1e-4\n                step = 0.18 * (temperature / 2.0)  # Slightly larger adaptive step\n                momentum = 0.6\n                trial_x = np.clip(current_x - step * grad_x + momentum * np.random.normal(0, 0.01), bounds[0], bounds[1])\n                trial_y = np.clip(current_y - step * grad_y + momentum * np.random.normal(0, 0.01), bounds[0], bounds[1])\n                trial_value = evaluate_function(trial_x, trial_y)\n                if trial_value < current_value:\n                    current_x, current_y, current_value = trial_x, trial_y, trial_value\n                    if current_value < best_value:\n                        best_x, best_y, best_value = current_x, current_y, current_value\n                    no_improve_count = 0\n\n            # Evolutionary jump: occasionally mix in a global crossover\n            if it % 30 == 0 and start > 0 and best_x is not None:\n                # Crossover between current and best found\n                alpha = np.random.uniform(0.2, 0.8)\n                cross_x = alpha * current_x + (1 - alpha) * best_x\n                cross_y = alpha * current_y + (1 - alpha) * best_y\n                cross_x = np.clip(cross_x, bounds[0], bounds[1])\n                cross_y = np.clip(cross_y, bounds[0], bounds[1])\n                cross_value = evaluate_function(cross_x, cross_y)\n                if cross_value < current_value:\n                    current_x, current_y, current_value = cross_x, cross_y, cross_value\n                    if current_value < best_value:\n                        best_x, best_y, best_value = current_x, current_y, current_value\n                    no_improve_count = 0\n\n            # If stuck, jump to a new random position (adaptive restart)\n            if no_improve_count > patience:\n                # Instead of random, jump near best found so far with noise\n                if best_x is not None and best_y is not None and np.isfinite(best_value):\n                    jump_x = best_x + np.random.normal(0, 1.3)\n                    jump_y = best_y + np.random.normal(0, 1.3)\n                    jump_x = np.clip(jump_x, bounds[0], bounds[1])\n                    jump_y = np.clip(jump_y, bounds[0], bounds[1])\n                    jump_value = evaluate_function(jump_x, jump_y)\n                    if jump_value < current_value:\n                        current_x, current_y, current_value = jump_x, jump_y, jump_value\n                    else:\n                        current_x = np.random.uniform(bounds[0], bounds[1])\n                        current_y = np.random.uniform(bounds[0], bounds[1])\n                        current_value = evaluate_function(current_x, current_y)\n                else:\n                    current_x = np.random.uniform(bounds[0], bounds[1])\n                    current_y = np.random.uniform(bounds[0], bounds[1])\n                    current_value = evaluate_function(current_x, current_y)\n                no_improve_count = 0\n\n            temperature = max(temperature * cooling_rate, min_temperature)\n\n    return best_x, best_y, best_value\n\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x*y) + (x**2 + y**2)/20\n# EVOLVE-BLOCK-END\n\n# This part remains fixed (not evolved)\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n    # The global minimum is around (-1.76, -1.03) with value -2.104\n", "language": "python", "parent_id": "5c6689f0-8227-4cd8-a461-c58cb10c9645", "generation": 6, "timestamp": 1747684512.2791836, "metrics": {"runs_successfully": 1.0, "value": -1.5152325589074755, "distance": 1.686923787679366, "value_score": 0.6294187394174849, "distance_score": 0.372172818814365, "overall_score": 0.5}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 74 lines with 104 lines", "parent_metrics": {"runs_successfully": 1.0, "value": -1.5127957119492823, "distance": 1.7520402516205702, "value_score": 0.6284548172158559, "distance_score": 0.3633667783060726, "overall_score": 0.5}}}