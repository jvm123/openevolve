{"id": "70276b82-e4ed-4d78-89f2-e76969f89438", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid global optimization using multistart simulated annealing and local search.\n    Improves exploration and escape from local minima.\n    \"\"\"\n    n_starts = 10  # Increase number of random restarts\n    best_x, best_y, best_value = None, None, np.inf\n\n    for start in range(n_starts):\n        # Initialize with a random point for each restart\n        current_x = np.random.uniform(bounds[0], bounds[1])\n        current_y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(current_x, current_y)\n        \n        temperature = 1.0\n        cooling_rate = 0.999  # Even slower cooling for more exploration\n        min_temperature = 1e-8  # Even lower minimum temperature for finer convergence\n\n        patience = 80\n        no_improve_count = 0\n\n        for it in range(iterations // n_starts):\n            # Generate a new candidate point by adding a perturbation\n            perturbation_scale = max(0.5 * temperature, 0.01)  # Increased scale for more exploration\n            new_x = current_x + np.random.normal(0, perturbation_scale)\n            new_y = current_y + np.random.normal(0, perturbation_scale)\n\n            # Enforce bounds\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n\n            new_value = evaluate_function(new_x, new_y)\n\n            # Acceptance criteria (Simulated Annealing)\n            if new_value < current_value:\n                accept = True\n            else:\n                accept = np.exp((current_value - new_value) / temperature) > np.random.rand()\n            \n            if accept:\n                current_x, current_y, current_value = new_x, new_y, new_value\n                no_improve_count = 0\n            else:\n                no_improve_count += 1\n\n            # Update best found so far\n            if current_value < best_value:\n                best_x, best_y, best_value = current_x, current_y, current_value\n                no_improve_count = 0\n\n            # Local search: occasionally make a \"greedy\" step\n            if it % 20 == 0:  # More frequent local search\n                grad_x = (evaluate_function(current_x + 1e-4, current_y) - current_value) / 1e-4\n                grad_y = (evaluate_function(current_x, current_y + 1e-4) - current_value) / 1e-4\n                step = 0.1  # Larger step size for more aggressive local search\n                trial_x = np.clip(current_x - step * grad_x, bounds[0], bounds[1])\n                trial_y = np.clip(current_y - step * grad_y, bounds[0], bounds[1])\n                trial_value = evaluate_function(trial_x, trial_y)\n                if trial_value < current_value:\n                    current_x, current_y, current_value = trial_x, trial_y, trial_value\n                    if current_value < best_value:\n                        best_x, best_y, best_value = current_x, current_y, current_value\n                    no_improve_count = 0\n\n            # If stuck, jump to a new random position (adaptive restart)\n            if no_improve_count > patience:\n                # Systematic exploration across the domain\n                for _ in range(5):  # Try five new random positions\n                    temp_x = np.random.uniform(bounds[0], bounds[1])\n                    temp_y = np.random.uniform(bounds[0], bounds[1])\n                    temp_value = evaluate_function(temp_x, temp_y)\n                    if temp_value < current_value:\n                        current_x, current_y, current_value = temp_x, temp_y, temp_value\n                        break\n                no_improve_count = 0\n\n            temperature = max(temperature * cooling_rate, min_temperature)\n\n    return best_x, best_y, best_value\n\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x*y) + (x**2 + y**2)/20\n# EVOLVE-BLOCK-END\n\n# This part remains fixed (not evolved)\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n    # The global minimum is around (-1.76, -1.03) with value -2.104\n", "language": "python", "parent_id": "1ebc6456-165b-4e7b-996d-f9ee2fd87198", "generation": 5, "timestamp": 1747684120.6977837, "metrics": {"runs_successfully": 1.0, "value": -1.5176602317293262, "distance": 1.7255297087131702, "value_score": 0.630381977431062, "distance_score": 0.3669011556920946, "overall_score": 0.5}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 2 lines with 2 lines\nChange 2: 'perturbation_scale = max(0.4 * temperature, 0.02)  # Larger scale for more jumps at high T' to 'perturbation_scale = max(0.5 * temperature, 0.01)  # Increased scale for more exploration'\nChange 3: 'step = 0.05' to 'step = 0.1  # Larger step size for more aggressive local search'\nChange 4: Replace 4 lines with 9 lines", "parent_metrics": {"runs_successfully": 1.0, "value": -1.5131907328984027, "distance": 1.7593274006760315, "value_score": 0.6286108716364014, "distance_score": 0.36240715753955155, "overall_score": 0.5}}}