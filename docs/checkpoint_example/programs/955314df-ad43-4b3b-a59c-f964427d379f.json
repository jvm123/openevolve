{"id": "955314df-ad43-4b3b-a59c-f964427d379f", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid global optimization using multistart simulated annealing and local search.\n    Improves exploration and escape from local minima.\n    \"\"\"\n    n_starts = 10  # Increase number of random restarts\n    best_x, best_y, best_value = None, None, np.inf\n\n    for start in range(n_starts):\n        # Initialize with a random point for each restart\n        current_x = np.random.uniform(bounds[0], bounds[1])\n        current_y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(current_x, current_y)\n        \n        temperature = 1.0\n        cooling_rate = 0.99  # Slower cooling for more exploration\n        min_temperature = 1e-8  # Even lower minimum temperature for finer convergence\n\n        patience = max(60, int(80 * (1 - temperature)))  # Dynamic patience based on temperature\n        no_improve_count = 0\n\n        for it in range(iterations // n_starts * 2):  # Increase iterations for deeper search in each restart\n            # Generate a new candidate point by adding a perturbation\n            perturbation_scale = max(0.5 * temperature, 0.01) * (1 + np.random.uniform(-0.2, 0.2))  # Increase and randomize perturbation scale more\n            new_x = current_x + np.random.normal(0, perturbation_scale)\n            new_y = current_y + np.random.normal(0, perturbation_scale)\n\n            # Enforce bounds\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n\n            new_value = evaluate_function(new_x, new_y)\n\n            # Acceptance criteria (Simulated Annealing)\n            if new_value < current_value:\n                accept = True\n            else:\n                accept = np.exp((current_value - new_value) / temperature) > np.random.rand()\n            \n            if accept:\n                current_x, current_y, current_value = new_x, new_y, new_value\n                no_improve_count = 0\n            else:\n                no_improve_count += 1\n\n            # Update best found so far\n            if current_value < best_value:\n                best_x, best_y, best_value = current_x, current_y, current_value\n                no_improve_count = 0\n\n            # Local search: occasionally make a \"greedy\" step\n            if it % 20 == 0:  # More frequent local search\n                grad_x = (evaluate_function(current_x + 1e-4, current_y) - current_value) / 1e-4\n                grad_y = (evaluate_function(current_x, current_y + 1e-4) - current_value) / 1e-4\n                step = 0.05\n                trial_x = np.clip(current_x - step * grad_x, bounds[0], bounds[1])\n                trial_y = np.clip(current_y - step * grad_y, bounds[0], bounds[1])\n                trial_value = evaluate_function(trial_x, trial_y)\n                if trial_value < current_value:\n                    current_x, current_y, current_value = trial_x, trial_y, trial_value\n                    if current_value < best_value:\n                        best_x, best_y, best_value = current_x, current_y, current_value\n                    no_improve_count = 0\n\n            # If stuck, jump to a new random position (adaptive restart)\n            if no_improve_count > patience or temperature < 0.05:\n                current_x = np.random.uniform(bounds[0], bounds[1])\n                current_y = np.random.uniform(bounds[0], bounds[1])\n                current_value = evaluate_function(current_x, current_y)\n                no_improve_count = 0\n\n            # Adaptive cooling schedule based on progress\n            if it % 50 == 0 and no_improve_count > patience // 2:\n                cooling_rate *= 0.9  # Slow down cooling if stuck\n            temperature = max(temperature * cooling_rate, min_temperature)\n\n    return best_x, best_y, best_value\n\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x*y) + (x**2 + y**2)/20\n# EVOLVE-BLOCK-END\n\n# This part remains fixed (not evolved)\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n    # The global minimum is around (-1.76, -1.03) with value -2.104\n", "language": "python", "parent_id": "2564795d-fa97-48bc-9235-9beda242ee38", "generation": 6, "timestamp": 1747684239.6475923, "metrics": {"runs_successfully": 1.0, "value": -1.518071800903431, "distance": 1.705403322863378, "value_score": 0.6305455698244438, "distance_score": 0.3696306541612463, "overall_score": 0.5}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 2 lines with 2 lines\nChange 2: 'for it in range(iterations // n_starts):' to 'for it in range(iterations // n_starts * 2):  # Increase iterations for deeper search in each restart'\nChange 3: 'perturbation_scale = max(0.4 * temperature, 0.02) * (1 + np.random.uniform(-0.1, 0.1))  # Randomize perturbation scale' to 'perturbation_scale = max(0.5 * temperature, 0.01) * (1 + np.random.uniform(-0.2, 0.2))  # Increase and randomize perturbation scale more'\nChange 4: Replace temperature = max(temperature * cooling_rate, min_temperature) with 4 lines", "parent_metrics": {"runs_successfully": 1.0, "value": -1.5137266826113325, "distance": 1.7586767044220888, "value_score": 0.6288227244119678, "distance_score": 0.36249263945899324, "overall_score": 0.5}}}