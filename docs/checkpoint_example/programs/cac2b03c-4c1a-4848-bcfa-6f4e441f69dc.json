{"id": "cac2b03c-4c1a-4848-bcfa-6f4e441f69dc", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\ndef search_algorithm(iterations=1200, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid global optimization using multistart simulated annealing, random search, evolutionary population, and local search.\n    Enhanced exploration, exploitation, and escape from local minima.\n    \"\"\"\n    n_starts = 25  # More restarts for wider global coverage\n    pop_size = 8   # Population-based search for diversity\n    elite_fraction = 0.25  # Keep top individuals\n    best_x, best_y, best_value = None, None, np.inf\n\n    for start in range(n_starts):\n        # Initialize a small population for each restart\n        population = []\n        for _ in range(pop_size):\n            ind_x = np.random.uniform(bounds[0], bounds[1])\n            ind_y = np.random.uniform(bounds[0], bounds[1])\n            ind_value = evaluate_function(ind_x, ind_y)\n            population.append([ind_x, ind_y, ind_value])\n\n        temperature = 2.0\n        cooling_rate = 0.991  # Slightly slower cooling for more exploration\n        min_temperature = 1e-10  # Allow even finer convergence\n\n        patience = 110\n        no_improve_count = 0\n        local_best_value = min(population, key=lambda ind: ind[2])[2]\n\n        for it in range(iterations // n_starts):\n            # Occasionally do a global random jump for some individuals\n            if it % 32 == 0 and np.random.rand() < 0.45:\n                idx = np.random.randint(pop_size)\n                rand_x = np.random.uniform(bounds[0], bounds[1])\n                rand_y = np.random.uniform(bounds[0], bounds[1])\n                rand_value = evaluate_function(rand_x, rand_y)\n                # Accept jump if better or with probability based on temperature\n                if rand_value < population[idx][2] or np.exp((population[idx][2] - rand_value) / (temperature+1e-12)) > np.random.rand():\n                    population[idx] = [rand_x, rand_y, rand_value]\n                    if rand_value < best_value:\n                        best_x, best_y, best_value = rand_x, rand_y, rand_value\n                    no_improve_count = 0\n\n            # Evolutionary/mutation step for population diversity\n            new_population = []\n            for ind in population:\n                # Candidate generation: Gaussian perturbation\n                perturbation_scale = max(0.45 * temperature, 0.01)\n                if np.random.rand() < 0.7:\n                    new_x = ind[0] + np.random.normal(0, perturbation_scale)\n                    new_y = ind[1] + np.random.normal(0, perturbation_scale)\n                else:\n                    # Crossover/combination with another random individual\n                    mate = population[np.random.randint(pop_size)]\n                    alpha = np.random.uniform(-0.2, 1.2)\n                    new_x = alpha * ind[0] + (1 - alpha) * mate[0]\n                    new_y = alpha * ind[1] + (1 - alpha) * mate[1]\n                    new_x += np.random.normal(0, perturbation_scale/2)\n                    new_y += np.random.normal(0, perturbation_scale/2)\n                new_x = np.clip(new_x, bounds[0], bounds[1])\n                new_y = np.clip(new_y, bounds[0], bounds[1])\n                new_value = evaluate_function(new_x, new_y)\n                # Simulated annealing acceptance\n                if new_value < ind[2]:\n                    accept = True\n                else:\n                    accept = np.exp((ind[2] - new_value) / (temperature+1e-12)) > np.random.rand()\n                if accept:\n                    new_population.append([new_x, new_y, new_value])\n                else:\n                    new_population.append(ind)\n\n            # Local search: gradient step on the current best in the population\n            if it % 9 == 0:\n                elite = sorted(new_population, key=lambda ind: ind[2])[:max(1,int(elite_fraction*pop_size))]\n                for e in elite:\n                    grad_eps = 1e-5\n                    grad_x = (evaluate_function(e[0] + grad_eps, e[1]) - e[2]) / grad_eps\n                    grad_y = (evaluate_function(e[0], e[1] + grad_eps) - e[2]) / grad_eps\n                    grad_norm = np.sqrt(grad_x**2 + grad_y**2) + 1e-8\n                    step = min(0.12, 0.7 * temperature/2) / grad_norm  # Slightly larger adaptive step\n                    trial_x = np.clip(e[0] - step * grad_x, bounds[0], bounds[1])\n                    trial_y = np.clip(e[1] - step * grad_y, bounds[0], bounds[1])\n                    trial_value = evaluate_function(trial_x, trial_y)\n                    if trial_value < e[2]:\n                        e[0], e[1], e[2] = trial_x, trial_y, trial_value\n\n            # Survivor selection: keep best, encourage diversity\n            population = sorted(new_population, key=lambda ind: ind[2])\n            # Replace worst with randoms to keep exploration\n            for i in range(int(pop_size*0.18)):\n                rand_x = np.random.uniform(bounds[0], bounds[1])\n                rand_y = np.random.uniform(bounds[0], bounds[1])\n                rand_value = evaluate_function(rand_x, rand_y)\n                population[-(i+1)] = [rand_x, rand_y, rand_value]\n            # Best solution in this generation\n            gen_best = population[0]\n            if gen_best[2] < best_value:\n                best_x, best_y, best_value = gen_best[0], gen_best[1], gen_best[2]\n                no_improve_count = 0\n            elif gen_best[2] < local_best_value:\n                local_best_value = gen_best[2]\n                no_improve_count = 0\n            else:\n                no_improve_count += 1\n\n            # If stuck, re-initialize part of the population (adaptive restart)\n            if no_improve_count > patience:\n                for i in range(int(pop_size//2)):\n                    population[i][0] = np.random.uniform(bounds[0], bounds[1])\n                    population[i][1] = np.random.uniform(bounds[0], bounds[1])\n                    population[i][2] = evaluate_function(population[i][0], population[i][1])\n                no_improve_count = 0\n\n            temperature = max(temperature * cooling_rate, min_temperature)\n\n    return best_x, best_y, best_value\n\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x*y) + (x**2 + y**2)/20\n# EVOLVE-BLOCK-END\n\n# This part remains fixed (not evolved)\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n    # The global minimum is around (-1.76, -1.03) with value -2.104\n", "language": "python", "parent_id": "97cdc700-2675-4141-87a5-dcf93f95bba0", "generation": 8, "timestamp": 1747683863.3988867, "metrics": {"runs_successfully": 1.0, "value": -1.5172406439068133, "distance": 1.7240316862310823, "value_score": 0.6302152851092263, "distance_score": 0.36710292507044245, "overall_score": 0.5}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 3 lines with 3 lines\nChange 2: 'pop_size = 8   # Population-based search for diversity' to 'pop_size = 10  # Increase initial population for better exploration'\nChange 3: Replace step = min(0.12, 0.7 * temperature/2) / grad_norm  # Slightly larger adaptive step with 2 lines", "parent_metrics": {"runs_successfully": 1.0, "value": -1.5185893457006194, "distance": 1.7014649692958308, "value_score": 0.6307514064499059, "distance_score": 0.37016952333853953, "overall_score": 0.5}}}