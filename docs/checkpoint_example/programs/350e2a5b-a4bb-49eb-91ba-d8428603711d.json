{"id": "350e2a5b-a4bb-49eb-91ba-d8428603711d", "code": "# EVOLVE-BLOCK-START\n\"\"\"Function minimization example for OpenEvolve\"\"\"\nimport numpy as np\n\ndef search_algorithm(iterations=1000, bounds=(-5, 5)):\n    \"\"\"\n    Hybrid global optimization using multistart simulated annealing, random search, and local search.\n    Enhanced exploration, exploitation, and escape from local minima.\n    \"\"\"\n    n_starts = 20  # Increase number of random restarts for more global coverage\n    best_x, best_y, best_value = None, None, np.inf\n\n    for start in range(n_starts):\n        # Initialize with a random point for each restart\n        current_x = np.random.uniform(bounds[0], bounds[1])\n        current_y = np.random.uniform(bounds[0], bounds[1])\n        current_value = evaluate_function(current_x, current_y)\n        \n        temperature = 2.0\n        cooling_rate = 0.992  # Even slower cooling for more exploration\n        min_temperature = 1e-8  # Allow more fine convergence\n\n        patience = 120\n        no_improve_count = 0\n\n        for it in range(iterations // n_starts):\n            # Occasionally do a global random jump to escape local minima\n            if it % 35 == 0 and np.random.rand() < 0.40:\n                rand_x = np.random.uniform(bounds[0], bounds[1])\n                rand_y = np.random.uniform(bounds[0], bounds[1])\n                rand_value = evaluate_function(rand_x, rand_y)\n                if rand_value < current_value or np.exp((current_value - rand_value) / (temperature+1e-12)) > np.random.rand():\n                    current_x, current_y, current_value = rand_x, rand_y, rand_value\n                    if current_value < best_value:\n                        best_x, best_y, best_value = current_x, current_y, current_value\n                    no_improve_count = 0\n\n            # Generate a new candidate point by adding a perturbation\n            perturbation_scale = max(0.7 * temperature, 0.01)  # Larger scale for more jumps at high T\n            new_x = current_x + np.random.normal(0, perturbation_scale)\n            new_y = current_y + np.random.normal(0, perturbation_scale)\n\n            # Enforce bounds\n            new_x = np.clip(new_x, bounds[0], bounds[1])\n            new_y = np.clip(new_y, bounds[0], bounds[1])\n\n            new_value = evaluate_function(new_x, new_y)\n\n            # Acceptance criteria (Simulated Annealing)\n            if new_value < current_value:\n                accept = True\n            else:\n                accept = np.exp((current_value - new_value) / (temperature+1e-12)) > np.random.rand()\n            \n            if accept:\n                current_x, current_y, current_value = new_x, new_y, new_value\n                no_improve_count = 0\n            else:\n                no_improve_count += 1\n\n            # Update best found so far\n            if current_value < best_value:\n                best_x, best_y, best_value = current_x, current_y, current_value\n                no_improve_count = 0\n\n            # Local search: hybridize with adaptive step gradient descent\n            if it % 12 == 0:  # More frequent local search\n                grad_eps = 1e-5\n                grad_x = (evaluate_function(current_x + grad_eps, current_y) - current_value) / grad_eps\n                grad_y = (evaluate_function(current_x, current_y + grad_eps) - current_value) / grad_eps\n                grad_norm = np.sqrt(grad_x**2 + grad_y**2) + 1e-8\n                step = min(0.08, 0.5 * temperature/2) / grad_norm  # Adaptive step size\n                trial_x = np.clip(current_x - step * grad_x, bounds[0], bounds[1])\n                trial_y = np.clip(current_y - step * grad_y, bounds[0], bounds[1])\n                trial_value = evaluate_function(trial_x, trial_y)\n                if trial_value < current_value:\n                    current_x, current_y, current_value = trial_x, trial_y, trial_value\n                    if current_value < best_value:\n                        best_x, best_y, best_value = current_x, current_y, current_value\n                    no_improve_count = 0\n\n            # If stuck, jump to a new random position (adaptive restart)\n            if no_improve_count > patience:\n                current_x = np.random.uniform(bounds[0], bounds[1])\n                current_y = np.random.uniform(bounds[0], bounds[1])\n                current_value = evaluate_function(current_x, current_y)\n                no_improve_count = 0\n\n            temperature = max(temperature * cooling_rate, min_temperature)\n\n    return best_x, best_y, best_value\n\ndef evaluate_function(x, y):\n    \"\"\"The complex function we're trying to minimize\"\"\"\n    return np.sin(x) * np.cos(y) + np.sin(x*y) + (x**2 + y**2)/20\n# EVOLVE-BLOCK-END\n\n# This part remains fixed (not evolved)\ndef run_search():\n    x, y, value = search_algorithm()\n    return x, y, value\n\nif __name__ == \"__main__\":\n    x, y, value = run_search()\n    print(f\"Found minimum at ({x}, {y}) with value {value}\")\n    # The global minimum is around (-1.76, -1.03) with value -2.104\n", "language": "python", "parent_id": "fbc34e4f-c22a-4e83-9d2b-3cbaa8ad7608", "generation": 6, "timestamp": 1747683574.8825357, "metrics": {"runs_successfully": 1.0, "value": -1.4989040660672401, "distance": 1.7365245964003113, "value_score": 0.6230157206552936, "distance_score": 0.36542700961483165, "overall_score": 0.5}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 74 lines with 87 lines", "parent_metrics": {"runs_successfully": 1.0, "value": -1.4981511396923075, "distance": 1.6149312762401669, "value_score": 0.6227236103704009, "distance_score": 0.3824192280256911, "overall_score": 0.5}}}